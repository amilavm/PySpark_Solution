{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tom</td>\n",
       "      <td>Male</td>\n",
       "      <td>67.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sarah</td>\n",
       "      <td>Female</td>\n",
       "      <td>61.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John</td>\n",
       "      <td>Male</td>\n",
       "      <td>80.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ann</td>\n",
       "      <td>Female</td>\n",
       "      <td>49.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jenny</td>\n",
       "      <td>Female</td>\n",
       "      <td>79.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name  gender   age  Experience\n",
       "0    Tom    Male  67.0        10.0\n",
       "1  Sarah  Female  61.0        12.0\n",
       "2   John    Male  80.0        14.0\n",
       "3    Ann  Female  49.0        13.0\n",
       "4  Jenny  Female  79.0        16.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_pd = pd.read_csv('sample_dataset.csv')\n",
    "df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "# you can give any name for session (eg: 'Practice')\n",
    "spark = SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.102:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe5cb24ffd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when you are executing in local, then always there will be only 1 cluster.\n",
    "But when you are working in cloud, you can create multiple clusters and instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('sample_dataset.csv')\n",
    "df_pyspark\n",
    "# here it will only prints the columns(but not as actual column names) and their data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+----------+\n",
      "|  _c0|   _c1| _c2|       _c3|\n",
      "+-----+------+----+----------+\n",
      "| Name|gender| age|Experience|\n",
      "|  Tom|  Male|  67|        10|\n",
      "|Sarah|Female|  61|        12|\n",
      "| John|  Male|  80|        14|\n",
      "|  Ann|Female|  49|        13|\n",
      "|Jenny|Female|  79|        16|\n",
      "| Jack|  Male|  81|        17|\n",
      "|Waugh|  null|null|        20|\n",
      "| Andy|  null|  69|        19|\n",
      "|Marie|  null|null|      null|\n",
      "+-----+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, gender: string, age: string, Experience: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark = spark.read.option('header', 'true').csv('sample_dataset.csv')\n",
    "df_pyspark\n",
    "# here it will prints the columns as actual column names and their data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+----------+\n",
      "| Name|gender| age|Experience|\n",
      "+-----+------+----+----------+\n",
      "|  Tom|  Male|  67|        10|\n",
      "|Sarah|Female|  61|        12|\n",
      "| John|  Male|  80|        14|\n",
      "|  Ann|Female|  49|        13|\n",
      "|Jenny|Female|  79|        16|\n",
      "| Jack|  Male|  81|        17|\n",
      "|Waugh|  null|null|        20|\n",
      "| Andy|  null|  69|        19|\n",
      "|Marie|  null|null|      null|\n",
      "+-----+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Tom', gender='Male', age='67', Experience='10'),\n",
       " Row(Name='Sarah', gender='Female', age='61', Experience='12')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printSchema is like df.info() in pandas; it tells about your columns\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform some basic operations\n",
    "\n",
    "    - Checking the Datatypes of the Column(Schema)\n",
    "    - Selecting Columns and Indexing\n",
    "    - Check Describe option similar to pandas\n",
    "    - Adding Columns\n",
    "    - Dropping Columns\n",
    "    - Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read the dataset\n",
    "df_pyspark = spark.read.option('header', 'true').csv('sample_dataset.csv', inferSchema=True)\n",
    "\n",
    "# here we need to configure 'inferSchema=True' in order to keep the datatypes of columns as in the csv, otherwise it will all appears as 'String' datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## check the schema\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or we can also read the csv as the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('sample_dataset.csv', header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## check the schema\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'gender', 'age', 'Experience']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get column names\n",
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Tom', gender='Male', age=67, Experience=10),\n",
       " Row(Name='Sarah', gender='Female', age=61, Experience=12)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(2)\n",
    "\n",
    "# this will give us the first 2 rows of data but not as dataframe but as a list, (but in pandas it will give us as a dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: int]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick only a single column (i.e. age)\n",
    "df_age = df_pyspark.select(\"age\")\n",
    "df_age\n",
    "\n",
    "# this will return the selected column as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|  67|\n",
      "|  61|\n",
      "|  80|\n",
      "|  49|\n",
      "|  79|\n",
      "|  81|\n",
      "|null|\n",
      "|  69|\n",
      "|null|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_age.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, age: int]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select two columns\n",
    "df_two_col = df_pyspark.select(['Name', 'age'])\n",
    "df_two_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| Name| age|\n",
      "+-----+----+\n",
      "|  Tom|  67|\n",
      "|Sarah|  61|\n",
      "| John|  80|\n",
      "|  Ann|  49|\n",
      "|Jenny|  79|\n",
      "| Jack|  81|\n",
      "|Waugh|null|\n",
      "| Andy|  69|\n",
      "|Marie|null|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_two_col.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'),\n",
       " ('gender', 'string'),\n",
       " ('age', 'int'),\n",
       " ('Experience', 'int')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check columns datatypes\n",
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for simplicity we take 'df_two_col' dataframe as it has only 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+\n",
      "|summary| Name|               age|\n",
      "+-------+-----+------------------+\n",
      "|  count|    9|                 7|\n",
      "|   mean| null| 69.42857142857143|\n",
      "| stddev| null|11.773659058213278|\n",
      "|    min| Andy|                49|\n",
      "|    max|Waugh|                81|\n",
      "+-------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe\n",
    "df_two_col.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, age: int, Age after 2 years: int]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding columns in data frames\n",
    "df_two_col = df_two_col.withColumn('Age after 2 years', df_two_col['age']+2)\n",
    "df_two_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----------------+\n",
      "| Name| age|Age after 2 years|\n",
      "+-----+----+-----------------+\n",
      "|  Tom|  67|               69|\n",
      "|Sarah|  61|               63|\n",
      "| John|  80|               82|\n",
      "|  Ann|  49|               51|\n",
      "|Jenny|  79|               81|\n",
      "| Jack|  81|               83|\n",
      "|Waugh|null|             null|\n",
      "| Andy|  69|               71|\n",
      "|Marie|null|             null|\n",
      "+-----+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_two_col.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| Name| age|\n",
      "+-----+----+\n",
      "|  Tom|  67|\n",
      "|Sarah|  61|\n",
      "| John|  80|\n",
      "|  Ann|  49|\n",
      "|Jenny|  79|\n",
      "| Jack|  81|\n",
      "|Waugh|null|\n",
      "| Andy|  69|\n",
      "|Marie|null|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop columns\n",
    "df_two_col = df_two_col.drop('Age after 2 years') # here we can also provide multiple column names to drop multiple columns\n",
    "df_two_col.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "| name| age|\n",
      "+-----+----+\n",
      "|  Tom|  67|\n",
      "|Sarah|  61|\n",
      "| John|  80|\n",
      "|  Ann|  49|\n",
      "|Jenny|  79|\n",
      "| Jack|  81|\n",
      "|Waugh|null|\n",
      "| Andy|  69|\n",
      "|Marie|null|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename the columns\n",
    "df_two_col = df_two_col.withColumnRenamed('Name', 'name')\n",
    "df_two_col.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark Handling Missing Values\n",
    "\n",
    " - Dropping Rows\n",
    " - Varoius parameter in dropping functionalities\n",
    " - Handling missing values by mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+----------+\n",
      "| Name|gender|age|Experience|\n",
      "+-----+------+---+----------+\n",
      "|  Tom|  Male| 67|        10|\n",
      "|Sarah|Female| 61|        12|\n",
      "| John|  Male| 80|        14|\n",
      "|  Ann|Female| 49|        13|\n",
      "|Jenny|Female| 79|        16|\n",
      "| Jack|  Male| 81|        17|\n",
      "+-----+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop all the rows with null/missing values\n",
    "df = df_pyspark\n",
    "df = df.na.drop()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters in Drop() functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there are different ways to drop null values using na.drop() function with configuring different parameters.\n",
    "\n",
    " 1. configure parameter '<b>how</b>':\n",
    "\n",
    "        how=any : drop rows if they even have a single null value \n",
    "        \n",
    "        or \n",
    "        \n",
    "        how=all : drop rows only if they completely have null values                     (it won't drop if it having any single non-null                          value)\n",
    "        \n",
    "        * by default this 'how' set to 'any' value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+----------+\n",
      "| Name|gender|age|Experience|\n",
      "+-----+------+---+----------+\n",
      "|  Tom|  Male| 67|        10|\n",
      "|Sarah|Female| 61|        12|\n",
      "| John|  Male| 80|        14|\n",
      "|  Ann|Female| 49|        13|\n",
      "|Jenny|Female| 79|        16|\n",
      "| Jack|  Male| 81|        17|\n",
      "+-----+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how==any\n",
    "# here we drop rows if they even have any single null value\n",
    "df = df_pyspark\n",
    "df = df.na.drop(how='any')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+----------+\n",
      "| Name|gender| age|Experience|\n",
      "+-----+------+----+----------+\n",
      "|  Tom|  Male|  67|        10|\n",
      "|Sarah|Female|  61|        12|\n",
      "| John|  Male|  80|        14|\n",
      "|  Ann|Female|  49|        13|\n",
      "|Jenny|Female|  79|        16|\n",
      "| Jack|  Male|  81|        17|\n",
      "|Waugh|  null|null|        20|\n",
      "| Andy|  null|  69|        19|\n",
      "|Marie|  null|null|      null|\n",
      "+-----+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how==all\n",
    "# here we drop rows if they completely have null values\n",
    "df = df_pyspark\n",
    "df = df.na.drop(how='all')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. configure threshold '<b>thresh</b>':\n",
    "\n",
    "       eg: \n",
    "       thresh=2 : \n",
    "       \n",
    "atleast 2 non-null values should be present\n",
    "(if any row has atleast 2 non-null values regardless of how many null values present it will not drop that row. \n",
    "\n",
    "But any row doesn't have atleast 2 non-null values, it will drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+----------+\n",
      "| Name|gender| age|Experience|\n",
      "+-----+------+----+----------+\n",
      "|  Tom|  Male|  67|        10|\n",
      "|Sarah|Female|  61|        12|\n",
      "| John|  Male|  80|        14|\n",
      "|  Ann|Female|  49|        13|\n",
      "|Jenny|Female|  79|        16|\n",
      "| Jack|  Male|  81|        17|\n",
      "|Waugh|  null|null|        20|\n",
      "| Andy|  null|  69|        19|\n",
      "+-----+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop rows if they don't have atleast 2 non-null values\n",
    "df = df_pyspark\n",
    "df = df.na.drop(how='any', thresh=2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Configure <b>subset</b>:\n",
    "\n",
    "    this used to drop rows based on null values in specific columns defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+----------+\n",
      "| Name|gender|age|Experience|\n",
      "+-----+------+---+----------+\n",
      "|  Tom|  Male| 67|        10|\n",
      "|Sarah|Female| 61|        12|\n",
      "| John|  Male| 80|        14|\n",
      "|  Ann|Female| 49|        13|\n",
      "|Jenny|Female| 79|        16|\n",
      "| Jack|  Male| 81|        17|\n",
      "| Andy|  null| 69|        19|\n",
      "+-----+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop rows if column 'age' has null values\n",
    "df = df_pyspark\n",
    "df = df.na.drop(how='any', subset=['age'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling missing values\n",
    "\n",
    "#### fill() method:\n",
    "\n",
    "this has 2 parameters;\n",
    "1. value: the value to be replaced\n",
    "2. subset: if any specific column(or columns) to be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----+----------+\n",
      "| Name|    gender| age|Experience|\n",
      "+-----+----------+----+----------+\n",
      "|  Tom|      Male|  67|        10|\n",
      "|Sarah|    Female|  61|        12|\n",
      "| John|      Male|  80|        14|\n",
      "|  Ann|    Female|  49|        13|\n",
      "|Jenny|    Female|  79|        16|\n",
      "| Jack|      Male|  81|        17|\n",
      "|Waugh|sample val|null|        20|\n",
      "| Andy|sample val|  69|        19|\n",
      "|Marie|sample val|null|      null|\n",
      "+-----+----------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill missing values with value of 'sample val'\n",
    "df = df_pyspark\n",
    "df = df.na.fill('sample val')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+----------+\n",
      "| Name|gender| age|Experience|\n",
      "+-----+------+----+----------+\n",
      "|  Tom|  Male|  67|        10|\n",
      "|Sarah|Female|  61|        12|\n",
      "| John|  Male|  80|        14|\n",
      "|  Ann|Female|  49|        13|\n",
      "|Jenny|Female|  79|        16|\n",
      "| Jack|  Male|  81|        17|\n",
      "|Waugh|  null|null|        20|\n",
      "| Andy|  null|  69|        19|\n",
      "|Marie|  null|null|      null|\n",
      "+-----+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill missing values with value of 'sample val' for 'age' column\n",
    "df = df_pyspark\n",
    "df = df.na.fill('sample val', 'age')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+----------+\n",
      "| Name|gender| age|Experience|\n",
      "+-----+------+----+----------+\n",
      "|  Tom|  Male|  67|        10|\n",
      "|Sarah|Female|  61|        12|\n",
      "| John|  Male|  80|        14|\n",
      "|  Ann|Female|  49|        13|\n",
      "|Jenny|Female|  79|        16|\n",
      "| Jack|  Male|  81|        17|\n",
      "|Waugh|  null|null|        20|\n",
      "| Andy|  null|  69|        19|\n",
      "|Marie|  null|null|      null|\n",
      "+-----+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill missing values with value of 'sample val' for multiple columns\n",
    "df = df_pyspark\n",
    "df = df.na.fill('sample val', ['age','Experience'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null values with mean\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['age', 'Experience'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience']]\n",
    "    ).setStrategy(\"mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+----------+-----------+------------------+\n",
      "| Name|gender| age|Experience|age_imputed|Experience_imputed|\n",
      "+-----+------+----+----------+-----------+------------------+\n",
      "|  Tom|  Male|  67|        10|         67|                10|\n",
      "|Sarah|Female|  61|        12|         61|                12|\n",
      "| John|  Male|  80|        14|         80|                14|\n",
      "|  Ann|Female|  49|        13|         49|                13|\n",
      "|Jenny|Female|  79|        16|         79|                16|\n",
      "| Jack|  Male|  81|        17|         81|                17|\n",
      "|Waugh|  null|null|        20|         69|                20|\n",
      "| Andy|  null|  69|        19|         69|                19|\n",
      "|Marie|  null|null|      null|         69|                15|\n",
      "+-----+------+----+----------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+----------+\n",
      "| Name|gender| age|Experience|\n",
      "+-----+------+----+----------+\n",
      "|  Tom|  Male|  67|        10|\n",
      "|Sarah|Female|  61|        12|\n",
      "| John|  Male|  80|        14|\n",
      "|  Ann|Female|  49|        13|\n",
      "|Jenny|Female|  79|        16|\n",
      "| Jack|  Male|  81|        17|\n",
      "|Waugh|  null|null|        20|\n",
      "| Andy|  null|  69|        19|\n",
      "|Marie|  null|null|      null|\n",
      "+-----+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+----------+\n",
      "| Name|gender|age|Experience|\n",
      "+-----+------+---+----------+\n",
      "|  Tom|  Male| 67|        10|\n",
      "|Sarah|Female| 61|        12|\n",
      "| John|  Male| 80|        14|\n",
      "|  Ann|Female| 49|        13|\n",
      "|Jenny|Female| 79|        16|\n",
      "| Jack|  Male| 81|        17|\n",
      "+-----+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df_pyspark\n",
    "df = df.na.drop()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+----------+\n",
      "| Name|gender|age|Experience|\n",
      "+-----+------+---+----------+\n",
      "|  Tom|  Male| 67|        10|\n",
      "|Sarah|Female| 61|        12|\n",
      "|  Ann|Female| 49|        13|\n",
      "+-----+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter records in which the age of the people less than or equal 70\n",
    "# 1st method\n",
    "df.filter('age<=70').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| Name|Experience|\n",
      "+-----+----------+\n",
      "|  Tom|        10|\n",
      "|Sarah|        12|\n",
      "|  Ann|        13|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if need only certain columns (i.e. Name, age) to display in filtered records\n",
    "df.filter('age<=70').select(['Name', 'Experience']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+----------+\n",
      "| Name|gender|age|Experience|\n",
      "+-----+------+---+----------+\n",
      "|  Tom|  Male| 67|        10|\n",
      "|Sarah|Female| 61|        12|\n",
      "|  Ann|Female| 49|        13|\n",
      "+-----+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter records in which the age of the people less than or equal 70\n",
    "# 2nd method\n",
    "df.filter(df['age']<=70).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+----------+\n",
      "| Name|gender|age|Experience|\n",
      "+-----+------+---+----------+\n",
      "|Jenny|Female| 79|        16|\n",
      "| Jack|  Male| 81|        17|\n",
      "+-----+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using multiple conditions\n",
    "df.filter((df['age']>=70) & \n",
    "          (df['Experience']>=15)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+----------+\n",
      "| Name|gender|age|Experience|\n",
      "+-----+------+---+----------+\n",
      "| John|  Male| 80|        14|\n",
      "|Jenny|Female| 79|        16|\n",
      "| Jack|  Male| 81|        17|\n",
      "+-----+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using multiple conditions\n",
    "df.filter((df['age']>=70) | \n",
    "          (df['Experience']>=15)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+----------+\n",
      "| Name|gender|age|Experience|\n",
      "+-----+------+---+----------+\n",
      "| John|  Male| 80|        14|\n",
      "|Jenny|Female| 79|        16|\n",
      "| Jack|  Male| 81|        17|\n",
      "+-----+------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inverse operation\n",
    "df.filter(~(df['age']<=70)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark GroupBy and Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp = spark.read.csv('sample_employee_dataset.csv', header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+------+\n",
      "| Name| Departments|Salary|\n",
      "+-----+------------+------+\n",
      "| Jane|Data Science| 10000|\n",
      "| Jane|         IOT|  5000|\n",
      "| Lisa|    Big Data|  4000|\n",
      "| Jane|    Big Data|  4000|\n",
      "| Lisa|Data Science|  3000|\n",
      "|  Roy|Data Science| 20000|\n",
      "|  Roy|         IOT| 10000|\n",
      "|  Roy|    Big Data|  5000|\n",
      "|Alexy|Data Science| 10000|\n",
      "|Alexy|    Big Data|  4000|\n",
      "+-----+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departments: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- groupby and aggregate works together\n",
    "\n",
    "- first we apply groupby and then we need to apply aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "| Name|sum(Salary)|\n",
      "+-----+-----------+\n",
      "|  Roy|      35000|\n",
      "|Alexy|      14000|\n",
      "| Jane|      19000|\n",
      "| Lisa|       7000|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group by 'Names' considering the sum of salaries\n",
    "# (here only column 'salary' has int values)\n",
    "df_emp.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "| Name|max(Salary)|\n",
      "+-----+-----------+\n",
      "|  Roy|      20000|\n",
      "|Alexy|      10000|\n",
      "| Jane|      10000|\n",
      "| Lisa|       4000|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# identify maximum salaries of each person\n",
    "df_emp.groupBy('Name').max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "| Name|min(Salary)|\n",
      "+-----+-----------+\n",
      "|  Roy|       5000|\n",
      "|Alexy|       4000|\n",
      "| Jane|       4000|\n",
      "| Lisa|       3000|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# identify min salaries of each person\n",
    "df_emp.groupBy('Name').min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|sum(Salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      15000|\n",
      "|    Big Data|      17000|\n",
      "|Data Science|      43000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Groupby Departments considering the sum of salaries\n",
    "df_emp.groupBy('Departments').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|avg(Salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|     7500.0|\n",
      "|    Big Data|     4250.0|\n",
      "|Data Science|    10750.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mean\n",
    "df_emp.groupBy('Departments').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#how many number of employees are working on each department\n",
    "df_emp.groupBy('Departments').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directly using aggregate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|      75000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the total value of salries\n",
    "df_emp.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
